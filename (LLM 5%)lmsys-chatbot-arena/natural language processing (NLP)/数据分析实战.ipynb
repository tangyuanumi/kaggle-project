{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据分析实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 读取数据\n",
    "\n",
    "    1. 先观察数据是否存在 “列名” 和 “index”\n",
    "\n",
    "    2. 选择csv或者Excel方式读取（尽量选择read_csv方法，可以减少内存消耗）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "\n",
    "data = pd.read_csv('AIDS_Classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 检查是否存在空缺值和异常值\n",
    "\n",
    "    1. 检测是否存在空缺值：isna()、isnull()方法 或者 missingno 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. isnull()方法 -- 检查每列的空缺值数量\n",
    "missing_values = data.isnull().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. isnull()方法 -- 检查每列的空缺值数量\n",
    "missing_values = data.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. missingno库\n",
    "import missingno as msno  \n",
    "\n",
    "msno.matrix(data)\n",
    "# msno.bar(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. 检查是否存在异常值：箱型图 或者 3σ 原则"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 箱型图方法\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# 设置图像大小\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 绘制箱型图\n",
    "data.boxplot()\n",
    "\n",
    "# 添加图表标题和轴标签\n",
    "plt.title('Box Plot of Variables')\n",
    "plt.ylabel('Values')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 3σ 原则\n",
    "means = data.mean()\n",
    "stds = data.std()\n",
    "\n",
    "for column in data.columns:\n",
    "    upper_bound = means[column] + 3 * stds[column]\n",
    "    lower_bound = means[column] - 3 * stds[column]\n",
    "    print(f\"{column} - Lower bound: {lower_bound}, Upper bound: {upper_bound}\")\n",
    "    print(\"Outliers:\")\n",
    "    print(data[(data[column] < lower_bound) | (data[column] > upper_bound)][column])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 相关性分析 （特征选择）\n",
    "\n",
    "**我们想知道哪些特征与标签的关系最大！**\n",
    "\n",
    "    1. 相关性分析（pearson线性相关性分析 / spearman相关性分析），但是，在进行pearson线性相关性分析前，我们首先需要检验每一列特征是否服从正态分布（高斯分布），因为，只有满足正态分布，pearson线性相关性分析才是有有意义的！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 数据分布检验（seaborn库可视化方法 —— 直方图）\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 设置图像大小\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 绘制每个特征的直方图\n",
    "for column in data.iloc[:, 0:4].columns:\n",
    "    sns.histplot(data[column], kde=True, label=column)\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(title='Variable')\n",
    "\n",
    "# 添加图表标题和轴标签\n",
    "plt.title('Histograms of Each Feature')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 正态分布检验（Q-Q图方法）\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "sm.qqplot(data['time'], line='45', fit=True)\n",
    "plt.title('Q-Q Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  1.正态分布检验（Shapro-Wilk检验 和 Kolmogorov-Smirnov检验）\n",
    "#  S-W检验方法一般需要样本量小于50，如果样本量大于50建议使用K-S检验\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# 对每列进行K-S检验\n",
    "for column in data.columns:\n",
    "    data_temp = data[column]\n",
    "    kstest_result = stats.kstest(data_temp, 'norm', args=(data_temp.mean(), data_temp.std()))\n",
    "    print(f\"Column {column}:\")\n",
    "    print(f\"  K-S Statistic: {kstest_result.statistic:.4f}, P-value: {kstest_result.pvalue:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 进行spearman相关性分析\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 计算Spearman相关系数矩阵\n",
    "spearman_corr = data.corr(method='spearman')\n",
    "\n",
    "# 设置热力图的大小\n",
    "plt.figure(figsize=(20, 12))\n",
    "\n",
    "# 绘制热力图\n",
    "sns.heatmap(spearman_corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "# 添加标题\n",
    "plt.title('Spearman Correlation Heatmap')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. 使用逻辑回归的方法分析相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 分离特征和标签\n",
    "X = data.drop(columns='infected')\n",
    "y = data['infected']\n",
    "\n",
    "# 实例化逻辑回归模型\n",
    "model = LogisticRegression()\n",
    "\n",
    "# 拟合模型\n",
    "model.fit(X, y)\n",
    "\n",
    "# 获取系数并创建系数DataFrame\n",
    "coefficients = pd.DataFrame(model.coef_[0], index=X.columns, columns=['Coefficient'])\n",
    "\n",
    "# 计算系数的绝对值并排序\n",
    "coefficients['Absolute Value'] = coefficients['Coefficient'].abs()\n",
    "coefficients = coefficients.sort_values(by='Absolute Value', ascending=False)\n",
    "\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. 使用随机森林的方法分析相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 分离特征和标签\n",
    "X = data.drop(columns='infected')\n",
    "y = data['infected']\n",
    "\n",
    "# 实例化随机森林模型\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 拟合模型\n",
    "rf.fit(X, y)\n",
    "\n",
    "# 获取特征重要性\n",
    "feature_importances = rf.feature_importances_\n",
    "\n",
    "# 创建特征重要性DataFrame并排序\n",
    "features_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': feature_importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# 可视化特征重要性\n",
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(features_df['Feature'], features_df['Importance'], color='b')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3种方法对比**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 分离特征和标签\n",
    "X = data.drop(columns='infected')\n",
    "y = data['infected']\n",
    "\n",
    "\n",
    "# 随机森林\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X, y)\n",
    "rf_importances = rf.feature_importances_\n",
    "\n",
    "# 逻辑回归\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_scaled, y)\n",
    "log_reg_coefs = np.abs(log_reg.coef_[0])\n",
    "\n",
    "# Spearman相关性\n",
    "spearman_corrs = np.abs(data.iloc[:, :-1].corrwith(data['infected'], method='spearman'))\n",
    "\n",
    "# 创建结果DataFrame\n",
    "result_df = pd.DataFrame({\n",
    "    'RandomForest': pd.Series(rf_importances, index=X.columns),\n",
    "    'LogisticRegression': pd.Series(log_reg_coefs, index=X.columns),\n",
    "    'SpearmanCorrelation': spearman_corrs\n",
    "})\n",
    "\n",
    "# 排序每列\n",
    "result_df = result_df.apply(lambda x: x.sort_values(ascending=False).index)\n",
    "result_df = result_df.reset_index()\n",
    "result_df.drop(columns='index',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**提取出3种方法共同的前10个中的特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取每种方法的前10个特征\n",
    "top10_rf = result_df['RandomForest'].head(10)\n",
    "top10_lr = result_df['LogisticRegression'].head(10)\n",
    "top10_spearman = result_df['SpearmanCorrelation'].head(10)\n",
    "\n",
    "# 计算三个列表的交集，找出共同的元素\n",
    "common_features = set(top10_rf).intersection(set(top10_lr)).intersection(set(top10_spearman))\n",
    "\n",
    "# 打印共同的特征\n",
    "print(\"共同的特征有：\", common_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 假设data是已经加载的DataFrame，common_features是已经定义的共同特征列表\n",
    "X_full = data.drop(columns='infected')\n",
    "y = data['infected']\n",
    "X_reduced = data[list(common_features)]\n",
    "\n",
    "# 数据标准化\n",
    "scaler_full = StandardScaler()\n",
    "X_full_scaled = scaler_full.fit_transform(X_full)\n",
    "\n",
    "scaler_reduced = StandardScaler()\n",
    "X_reduced_scaled = scaler_reduced.fit_transform(X_reduced)\n",
    "\n",
    "# 分割数据集\n",
    "X_train_full, X_test_full, y_train, y_test = train_test_split(X_full_scaled, y, test_size=0.3, random_state=42)\n",
    "X_train_red, X_test_red, _, _ = train_test_split(X_reduced_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 定义模型\n",
    "models = {\n",
    "    'SVM': SVC(probability=True),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(max_depth=5),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=1000),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'LightGBM': LGBMClassifier(max_depth=5,n_estimators=1000)\n",
    "}\n",
    "\n",
    "# 绘制每个模型的ROC曲线\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for ax, (name, model) in zip(axes, models.items()):\n",
    "    # 训练并预测完整特征集\n",
    "    model.fit(X_train_full, y_train)\n",
    "    preds_full = model.predict_proba(X_test_full)[:, 1]\n",
    "    fpr_full, tpr_full, _ = roc_curve(y_test, preds_full)\n",
    "    roc_auc_full = auc(fpr_full, tpr_full)\n",
    "    \n",
    "    # 训练并预测缩减特征集\n",
    "    model.fit(X_train_red, y_train)\n",
    "    preds_red = model.predict_proba(X_test_red)[:, 1]\n",
    "    fpr_red, tpr_red, _ = roc_curve(y_test, preds_red)\n",
    "    roc_auc_red = auc(fpr_red, tpr_red)\n",
    "    \n",
    "    # 绘制ROC曲线\n",
    "    ax.plot(fpr_full, tpr_full, label=f'Full Features (AUC = {roc_auc_full:.2f})', color='blue')\n",
    "    ax.plot(fpr_red, tpr_red, label=f'Common Features (AUC = {roc_auc_red:.2f})', color='red')\n",
    "    ax.plot([0, 1], [0, 1], 'k--')\n",
    "    ax.set_xlim([0.0, 1.0])\n",
    "    ax.set_ylim([0.0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate')\n",
    "    ax.set_ylabel('True Positive Rate')\n",
    "    ax.set_title(f'ROC Curve for {name}')\n",
    "    ax.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 数据标准化简介\n",
    "\n",
    "    数据标准化是一种预处理技术，用于将数据调整到一个共同的规模，通常用于改善算法的性能和结果的可靠性。它特别重要对于那些依赖于数据尺度的机器学习算法，例如支持向量机（SVM）和最近邻（KNN），因为这些算法会基于数据点之间的距离来作出决策。\n",
    "\n",
    "#### 常见的数据标准化方法\n",
    "\n",
    "1. **最小-最大缩放（Min-Max Scaling）**：\n",
    "   - 将数据缩放到一个指定的范围，通常是0到1，或者是-1到1。\n",
    "   - 公式：\n",
    "   $$ \n",
    "   X_{\\text{norm}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \n",
    "   $$\n",
    "   - 其中$ X_{\\text{min}} $和$ X_{\\text{max}} $分别是数据在该特征上的最小值和最大值。\n",
    "   - 适应场景：当你需要数据严格限定在一定范围内时。\n",
    "\n",
    "2. **Z得分标准化（Z-Score Normalization）**：\n",
    "   - 也称为标准化或标准分数，它会将数据的均值调整为0，标准差调整为1。\n",
    "   - 公式：\n",
    "   $$ \n",
    "   X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}\n",
    "   $$\n",
    "   - 其中$ \\mu $是样本均值，$ \\sigma $是样本标准差。\n",
    "   - 适应场景：尤其适用于数据近似呈正态分布时。\n",
    "\n",
    "3. **最大绝对值缩放（MaxAbs Scaling）**：\n",
    "   - 将数据除以数据的最大绝对值以将每个特征缩放到[-1, 1]范围内，不移动/不居中数据。\n",
    "   - 公式：\n",
    "   $$\n",
    "    X_{\\text{scaled}} = \\frac{X}{\\max(\\left|X\\right|)}\n",
    "   $$\n",
    "   - 适应场景：处理稀疏数据时非常有用（不会破坏稀疏性）。\n",
    "\n",
    "4. **单位长度缩放（Normalization to Unit Length）**：\n",
    "   - 调整数据的尺度使得特征向量的欧氏长度（L2范数）为1。\n",
    "   - 公式：\n",
    "   $$ \n",
    "   X_{\\text{normalized}} = \\frac{X}{\\left\\| X \\right\\|_2}\n",
    "   $$\n",
    "   - 适应场景：有时用于文本分类和聚类，以及在使用点积计算相似度时。\n",
    "\n",
    "#### 数据标准化的重要性\n",
    "\n",
    "- **改进学习算法的收敛速度**：在梯度下降等优化算法中，如果一个特征的范围远大于另一个特征的范围，它可能会主导梯度下降的方向，使得学习过程变慢。\n",
    "- **避免数值不稳定性**：大的数值范围可能导致数值计算上的不稳定，如溢出或下溢。\n",
    "- **提高模型的准确度**：特别是对于基于距离的算法，如KNN、SVM，标准化可以防止某些特征因为尺度大而对距离计算产生过大影响。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
